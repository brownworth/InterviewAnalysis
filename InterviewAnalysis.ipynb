{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import docx2txt\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer, SnowballStemmer\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two cells are probably not needed after the first run. They just convert the word files to text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not os.path.exists('text'):\n",
    "#     os.makedirs('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for word_doc in os.listdir('./docx/'):\n",
    "#     filename,ext = os.path.splitext(word_doc)\n",
    "#     if ext == \".docx\":\n",
    "#         raw_text = docx2txt.process(f'./docx/{word_doc}')\n",
    "#         with(open(f'./text/{filename}.txt','w')) as text_file:\n",
    "#             text_file.write(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "filenames = []\n",
    "for text_doc in os.listdir('./text'):\n",
    "    filename,ext = os.path.splitext(text_doc)\n",
    "    if ext == \".txt\":\n",
    "        filenames.append(f'{text_doc}')\n",
    "        with(open(f'./text/{text_doc}','r',encoding=\"utf8\", errors='ignore')) as f:\n",
    "            corpus.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc_test = \"Hel!lo I a?m A String)(#*) (#*!@)#(*!)@(*Filled With (*$(*#)))Punctuation\"\n",
    "punc_test.translate(punc_test.maketrans('','',string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub(\"[^A-Za-z0-9 ]\",\"\",punc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit(re.sub(\"[^A-Za-z0-9 ]\",\"\",punc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit(punc_test.translate(punc_test.maketrans('','',string.punctuation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_set = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stops = set(pd.read_csv('custom_stop.csv',header=None,squeeze=True).values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_with_custom = stop_words_set.union(custom_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(custom_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'because', 'doing', 'have', 'having', 'just', 'ma'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words_set.intersection(custom_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "285"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_with_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Component                                                     | Code  |\n",
    "|---------------------------------------------------------------|-------|\n",
    "| TFIDF/Count                                                   | T/C   |\n",
    "| Stemming/Lemmatization/Base                                   | S/L/B |\n",
    "| Punctuation/Alphanumeric Only                                 | P/A   |\n",
    "| Letters / Numbers / Word-bounded Numbers                      | L/N/W |\n",
    "| 1-, 2-, 3-grams                                               | 1/2/3 |\n",
    "| No Stopwords/Standard Stopwords/Standard and Custom Stopwords | N/S/C |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table above shows how a file will be coded. For example, the file for document 36, using TFIDF, with Stemming, only Alphanumeric Characters (no punctuation), Letters and Numbers, max n-gram size of 3, using the custom and standard stopword lists is `36-TSAN3C.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub(r\"\\b\\d+\\b\",'',\"6 pieces of 35mm film in 5 lb containers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub(\"\\d+\",'',\"6 pieces of 35mm film in 5 lb containers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer.lemmatize(\"better\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer.lemmatize(\"better\",pos=\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer.lemmatize(\"went\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer.lemmatize(\"went\",pos=\"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbstemmer.stem(\"visually\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmer = PorterStemmer()\n",
    "# sbstemmer = SnowballStemmer(\"english\")\n",
    "# sbstemmer = SnowballStemmer(\"english\",ignore_stopwords=True)\n",
    "# lemmer = WordNetLemmatizer()\n",
    "# def custom_preprocessor(token):\n",
    "#     new_token = token\n",
    "#     new_token = new_token.translate(new_token.maketrans('','',string.punctuation))\n",
    "#     new_token = re.sub(\"[^A-Za-z0-9 ]\",\"\",new_token)\n",
    "#     new_token = stemmer.stem(new_token)\n",
    "#     new_token = sbstemmer.stem(new_token)\n",
    "#     new_token = lemmer.lemmatize(new_token)\n",
    "#     return new_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbstemmer = SnowballStemmer(\"english\")\n",
    "def custom_preprocessor(token):\n",
    "    new_token = token\n",
    "    new_token = re.sub(\"[^A-Za-z0-9 ]\",\"\",new_token)\n",
    "    new_token = sbstemmer.stem(new_token)\n",
    "    return new_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_preprocessor(\"v'i's'u'a'l'l'y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stops_proc = [custom_preprocessor(word) for word in stop_with_custom]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stops_proc.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'abl',\n",
       " 'about',\n",
       " 'abov',\n",
       " 'actual',\n",
       " 'add',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ago',\n",
       " 'ain',\n",
       " 'aka',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'ani',\n",
       " 'are',\n",
       " 'aren',\n",
       " 'arent',\n",
       " 'arent',\n",
       " 'arguabl',\n",
       " 'as',\n",
       " 'at',\n",
       " 'ba',\n",
       " 'basic',\n",
       " 'be',\n",
       " 'be',\n",
       " 'becaus',\n",
       " 'been',\n",
       " 'befor',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'cant',\n",
       " 'certain',\n",
       " 'come',\n",
       " 'consider',\n",
       " 'couldn',\n",
       " 'couldnt',\n",
       " 'current',\n",
       " 'current',\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " 'didnt',\n",
       " 'do',\n",
       " 'do',\n",
       " 'doc',\n",
       " 'doe',\n",
       " 'doesn',\n",
       " 'doesnt',\n",
       " 'doesnt',\n",
       " 'doi',\n",
       " 'don',\n",
       " 'dont',\n",
       " 'dont',\n",
       " 'down',\n",
       " 'dure',\n",
       " 'each',\n",
       " 'eg',\n",
       " 'els',\n",
       " 'especi',\n",
       " 'etc',\n",
       " 'exampl',\n",
       " 'exampl',\n",
       " 'far',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'general',\n",
       " 'general',\n",
       " 'given',\n",
       " 'go',\n",
       " 'goe',\n",
       " 'good',\n",
       " 'got',\n",
       " 'ha',\n",
       " 'had',\n",
       " 'hadn',\n",
       " 'hadnt',\n",
       " 'han',\n",
       " 'has',\n",
       " 'hasn',\n",
       " 'hasnt',\n",
       " 'have',\n",
       " 'have',\n",
       " 'haven',\n",
       " 'havent',\n",
       " 'havent',\n",
       " 'he',\n",
       " 'her',\n",
       " 'her',\n",
       " 'here',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'http',\n",
       " 'https',\n",
       " 'i',\n",
       " 'id',\n",
       " 'ie',\n",
       " 'if',\n",
       " 'ill',\n",
       " 'im',\n",
       " 'in',\n",
       " 'includ',\n",
       " 'incred',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " 'isnt',\n",
       " 'isnt',\n",
       " 'issu',\n",
       " 'it',\n",
       " 'it',\n",
       " 'it',\n",
       " 'it',\n",
       " 'itself',\n",
       " 'ive',\n",
       " 'just',\n",
       " 'like',\n",
       " 'like',\n",
       " 'likewis',\n",
       " 'll',\n",
       " 'lot',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'mayb',\n",
       " 'me',\n",
       " 'mean',\n",
       " 'mightn',\n",
       " 'mightnt',\n",
       " 'more',\n",
       " 'most',\n",
       " 'multi',\n",
       " 'mustn',\n",
       " 'mustnt',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'na',\n",
       " 'need',\n",
       " 'needn',\n",
       " 'neednt',\n",
       " 'new',\n",
       " 'no',\n",
       " 'non',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'obvious',\n",
       " 'of',\n",
       " 'off',\n",
       " 'oh',\n",
       " 'on',\n",
       " 'onc',\n",
       " 'one',\n",
       " 'onli',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'our',\n",
       " 'ourselv',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'peopl',\n",
       " 'quit',\n",
       " 'r1',\n",
       " 'ra',\n",
       " 're',\n",
       " 'realli',\n",
       " 'regard',\n",
       " 'regardless',\n",
       " 'resourc',\n",
       " 's',\n",
       " 'sa',\n",
       " 'said',\n",
       " 'same',\n",
       " 'say',\n",
       " 'shan',\n",
       " 'shant',\n",
       " 'she',\n",
       " 'shes',\n",
       " 'should',\n",
       " 'shouldn',\n",
       " 'shouldnt',\n",
       " 'shouldv',\n",
       " 'so',\n",
       " 'some',\n",
       " 'special',\n",
       " 'specif',\n",
       " 'specif',\n",
       " 'such',\n",
       " 'sure',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " 'that',\n",
       " 'thatll',\n",
       " 'the',\n",
       " 'their',\n",
       " 'their',\n",
       " 'them',\n",
       " 'themselv',\n",
       " 'then',\n",
       " 'there',\n",
       " 'there',\n",
       " 'thereof',\n",
       " 'these',\n",
       " 'they',\n",
       " 'theyr',\n",
       " 'theyv',\n",
       " 'think',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 'varieti',\n",
       " 'various',\n",
       " 've',\n",
       " 'veri',\n",
       " 'vs',\n",
       " 'was',\n",
       " 'wasn',\n",
       " 'wasnt',\n",
       " 'wasnt',\n",
       " 'way',\n",
       " 'way',\n",
       " 'we',\n",
       " 'were',\n",
       " 'were',\n",
       " 'weren',\n",
       " 'werent',\n",
       " 'weve',\n",
       " 'what',\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'whew',\n",
       " 'whi',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'whos',\n",
       " 'wi',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " 'wont',\n",
       " 'wont',\n",
       " 'wouldn',\n",
       " 'wouldnt',\n",
       " 'wouldnt',\n",
       " 'www',\n",
       " 'wwwalaorg',\n",
       " 'wwwlifesciedorg',\n",
       " 'y',\n",
       " 'yes',\n",
       " 'yesterday',\n",
       " 'york',\n",
       " 'you',\n",
       " 'youd',\n",
       " 'youll',\n",
       " 'your',\n",
       " 'your',\n",
       " 'your',\n",
       " 'your',\n",
       " 'yourself',\n",
       " 'yourselv',\n",
       " 'youv',\n",
       " 'zen',\n",
       " 'zillion']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_stops_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_with_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_code = \"TSAN3C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vect = TfidfVectorizer(ngram_range=(1,3),stop_words=stop_with_custom,preprocessor=custom_preprocessor)\n",
    "text_proc = text_vect.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(text_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_array = text_proc.todense()\n",
    "tfidf_values_list = tfidf_array.tolist()\n",
    "tfidf_lists = pd.DataFrame(tfidf_values_list,\n",
    "                           columns=text_vect.get_feature_names(),\n",
    "                           index=filenames).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f'csv/{type_code}'):\n",
    "    os.makedirs(f'csv/{type_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column_name in tfidf_lists.columns:\n",
    "#     new_file = column_name.replace('txt','csv')\n",
    "    new_file = f\"{column_name[:-4]}-{type_code}.csv\"\n",
    "#     print(new_file)\n",
    "    tfidf_lists[tfidf_lists[column_name]>0][column_name].sort_values(ascending=False).to_csv(f'./csv/{type_code}/{new_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_code = \"CSAN3C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brown/.local/share/virtualenvs/python-mWUFCbuH/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abl', 'abov', 'actual', 'ani', 'arent', 'arguabl', 'basic', 'becaus', 'befor', 'cant', 'certain', 'come', 'consider', 'couldnt', 'didnt', 'doe', 'doesnt', 'dont', 'dure', 'eg', 'els', 'especi', 'exampl', 'go', 'goe', 'hadnt', 'hasnt', 'havent', 'id', 'ie', 'ill', 'im', 'includ', 'incred', 'isnt', 'issu', 'ive', 'likewis', 'mayb', 'mean', 'mightnt', 'mustnt', 'neednt', 'obvious', 'onc', 'one', 'onli', 'ourselv', 'peopl', 'quit', 'ra', 'realli', 'regard', 'resourc', 'shant', 'shes', 'shouldnt', 'shouldv', 'special', 'specif', 'thatll', 'themselv', 'theyr', 'theyv', 'varieti', 'veri', 'wasnt', 'werent', 'weve', 'whi', 'whos', 'wont', 'wouldnt', 'wwwalaorg', 'wwwlifesciedorg', 'youd', 'youll', 'yourselv', 'youv'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "text_vect = CountVectorizer(ngram_range=(1,3),stop_words=stop_with_custom,preprocessor=custom_preprocessor)\n",
    "text_proc = text_vect.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_array = text_proc.todense()\n",
    "count_values_list = count_array.tolist()\n",
    "count_lists = pd.DataFrame(count_values_list,\n",
    "                           columns=text_vect.get_feature_names(),\n",
    "                           index=filenames).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f'csv/{type_code}'):\n",
    "    os.makedirs(f'csv/{type_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column_name in tfidf_lists.columns:\n",
    "#     new_file = column_name.replace('txt','csv')\n",
    "    new_file = f\"{column_name[:-4]}-{type_code}.csv\"\n",
    "#     print(new_file)\n",
    "    count_lists[count_lists[column_name]>0][column_name].sort_values(ascending=False).to_csv(f'./csv/{type_code}/{new_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[LDA Stuff](https://towardsdatascience.com/2-latent-methods-for-dimension-reduction-and-topic-modeling-20ff6d7d547)\n",
    "[LDA Stuff 2](https://medium.com/analytics-vidhya/topic-modelling-using-latent-dirichlet-allocation-in-scikit-learn-7daf770406c4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cvect = CountVectorizer(max_df = 0.95, min_df = 2, stop_words = stop_words)\n",
    "text_cproc = text_cvect.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lda = LatentDirichletAllocation(n_components = 10, random_state = 42)\n",
    "text_lda.fit(text_cproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, topic in enumerate(text_lda.components_):\n",
    "    print(f'Top 15 words for Topic #{index}')\n",
    "    print([text_cvect.get_feature_names()[i] for i in topic.argsort()[-15:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
