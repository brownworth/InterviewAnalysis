{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import docx2txt\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two cells are probably not needed after the first run. They just convert the word files to text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('text'):\n",
    "    os.makedirs('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word_doc in os.listdir('./docx/'):\n",
    "    filename,ext = os.path.splitext(word_doc)\n",
    "    if ext == \".docx\":\n",
    "        raw_text = docx2txt.process(f'./docx/{word_doc}')\n",
    "        with(open(f'./text/{filename}.txt','w')) as text_file:\n",
    "            text_file.write(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "filenames = []\n",
    "for text_doc in os.listdir('./text'):\n",
    "    filename,ext = os.path.splitext(text_doc)\n",
    "    if ext == \".txt\":\n",
    "        filenames.append(f'{text_doc}')\n",
    "        with(open(f'./text/{text_doc}','r',encoding=\"utf8\", errors='ignore')) as f:\n",
    "            corpus.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vect = TfidfVectorizer(ngram_range=(1,2),stop_words=stop_words)\n",
    "text_proc = text_vect.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_array = text_proc.todense()\n",
    "tfidf_values_list = tfidf_array.tolist()\n",
    "tfidf_lists = pd.DataFrame(tfidf_values_list,\n",
    "                           columns=text_vect.get_feature_names(),\n",
    "                           index=filenames).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_lists.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('csv'):\n",
    "    os.makedirs('csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column_name in tfidf_lists.columns:\n",
    "    new_file = column_name.replace('txt','csv')\n",
    "    tfidf_lists[tfidf_lists[column_name]>0][column_name].sort_values(ascending=False).to_csv(f'./csv/{new_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[LDA Stuff](https://towardsdatascience.com/2-latent-methods-for-dimension-reduction-and-topic-modeling-20ff6d7d547)\n",
    "[LDA Stuff 2](https://medium.com/analytics-vidhya/topic-modelling-using-latent-dirichlet-allocation-in-scikit-learn-7daf770406c4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cvect = CountVectorizer(max_df = 0.95, min_df = 2, stop_words = stop_words)\n",
    "text_cproc = text_cvect.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lda = LatentDirichletAllocation(n_components = 10, random_state = 42)\n",
    "text_lda.fit(text_cproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, topic in enumerate(text_lda.components_):\n",
    "    print(f'Top 15 words for Topic #{index}')\n",
    "    print([text_cvect.get_feature_names()[i] for i in topic.argsort()[-15:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lda.components_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
